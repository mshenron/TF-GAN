{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Discriminator Net\n",
    "def genrator(z):\n",
    "    with tf.variable_scope('GEN',reuse=tf.AUTO_REUSE):\n",
    "        G_h1=tf.contrib.layers.fully_connected(inputs=z, num_outputs=128, activation_fn=tf.nn.relu,weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        G_log_prob=tf.contrib.layers.fully_connected(inputs=G_h1,num_outputs=784,activation_fn=None,weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        G_prob=tf.nn.sigmoid(G_log_prob,name='G_prob')\n",
    "    return G_prob\n",
    "\n",
    "#Genrator Net\n",
    "def discriminator(x):\n",
    "    with tf.variable_scope('DES',reuse=tf.AUTO_REUSE):\n",
    "        D_h1=tf.contrib.layers.fully_connected(inputs=x,num_outputs=128,activation_fn=tf.nn.relu,weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        D_logits=tf.contrib.layers.fully_connected(inputs=D_h1,num_outputs=1,activation_fn=None,weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        D_prob=tf.nn.sigmoid(D_logits)\n",
    "    return D_prob,D_logits\n",
    "\n",
    "#uniform prior for sample\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "#\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    gs = gridspec.GridSpec(5, 5)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chkpt_dir='chkpt000'\n",
    "batch_size=128\n",
    "z_dim=100\n",
    "tf.reset_default_graph()\n",
    "experiment_id=0\n",
    "T_graph=tf.Graph()\n",
    "batch_count = int(mnist.train.num_examples/batch_size)\n",
    "with T_graph.as_default():\n",
    "    #Z=tf.constant(np.random.random([10,100]),dtype=tf.float32,name='Z')\n",
    "    #X=tf.constant(np.random.random([10,784]),dtype=tf.float32,name='X')\n",
    "    X=tf.placeholder(tf.float32,shape=[None,784],name='X')\n",
    "    Z=tf.placeholder(tf.float32,shape=[None,100],name='X')\n",
    "    G_sample=genrator(Z)\n",
    "    D_real,D_logit_real=discriminator(X)\n",
    "    D_fake,D_logit_fake=discriminator(G_sample)\n",
    "    D_loss=-tf.reduce_mean(tf.log(D_real)+tf.log(1.0-D_fake))\n",
    "    G_loss=-tf.reduce_mean(tf.log(D_fake))\n",
    "    gen_vars=[var for var in tf.trainable_variables() if var.name[:3]=='GEN']\n",
    "    des_vars=[var for var in tf.trainable_variables() if var.name[:3]=='DES']\n",
    "    D_optimize=tf.train.AdamOptimizer(learning_rate=0.0001,name='D_Adam').minimize(D_loss,var_list=des_vars)\n",
    "    G_optimize=tf.train.AdamOptimizer(learning_rate=0.0001,name='G_Adam').minimize(G_loss,var_list=gen_vars)\n",
    "    \n",
    "    #summaries=set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "    D_summaries=set([])\n",
    "    D_summaries.add(tf.summary.scalar('D_loss',D_loss))\n",
    "    D_summary_op=tf.summary.merge(list(D_summaries))\n",
    "    \n",
    "    G_summaries=set([])\n",
    "    G_summaries.add(tf.summary.scalar('G_loss',G_loss))\n",
    "    G_summary_op=tf.summary.merge(list(G_summaries))\n",
    "    \n",
    "with tf.Session(graph=T_graph) as sess:\n",
    "    saver=tf.train.Saver()\n",
    "    summary_writer=tf.summary.FileWriter(chkpt_dir,graph=T_graph)\n",
    "    init=[tf.global_variables_initializer(),tf.local_variables_initializer()]\n",
    "    sess.run(init)\n",
    "    tf_vars=tf.trainable_variables()\n",
    "    if os.path.exists(chkpt_dir+'/checkpoint'):\n",
    "        print('Found Chcekpoint')\n",
    "        saver.restore(sess,chkpt_dir+'/model.chkpt')\n",
    "        print('checkpoint_restored !')\n",
    "    elif not os.path.exists(chkpt_dir):\n",
    "        os.mkdir(chkpt_dir)\n",
    "    print('Trainng Started...')\n",
    "    #p_bar=tqdm(100000,desc='G_loss : N/A ; D_loss : N/A')\n",
    "    step_D_loss,step_G_loss=[],[]\n",
    "    for i in tqdm(range(1000000)):\n",
    "        #p_bar.update(1)\n",
    "        X_real, _ = mnist.train.next_batch(batch_size)\n",
    "        _, D_loss_curr,D_summary = sess.run([D_optimize, D_loss,D_summary_op], feed_dict={X: X_real, Z: sample_Z(batch_size, z_dim)})\n",
    "        _, G_loss_curr,G_summary = sess.run([G_optimize, G_loss,G_summary_op], feed_dict={Z: sample_Z(batch_size, z_dim)})\n",
    "        step_D_loss.append(D_loss_curr)\n",
    "        step_G_loss.append(G_loss_curr)\n",
    "        summary_writer.add_summary(D_summary, i)\n",
    "        summary_writer.add_summary(G_summary, i)\n",
    "        if i% 1000==0:\n",
    "            #p_bar.set_description('G_loss : {} ; D_loss : {}'.format((sum(step_D_loss)/len(step_D_loss)),(sum(step_G_loss)/len(step_G_loss))))\n",
    "            step_D_loss,step_G_loss=[],[]\n",
    "            samples=sess.run(G_sample,feed_dict={Z:sample_Z(25,z_dim)})\n",
    "            fig=plot(samples)\n",
    "            plt.savefig('out/{}.png'.format(str(experiment_id).zfill(3)+'_'+str(i/1000).zfill(5)))\n",
    "            plt.close(fig)\n",
    "            saver.save(sess,chkpt_dir+'/model.chkpt')\n",
    "    print('G_loss : {} ; D_loss : {}'.format((sum(step_D_loss)/len(step_D_loss)),(sum(step_G_loss)/len(step_G_loss))))\n",
    "    print('training Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Alternative losses:\n",
    "# D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(D_logit_real, tf.ones_like(D_logit_real)))\n",
    "# D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(D_logit_fake, tf.zeros_like(D_logit_fake)))\n",
    "# D_loss = D_loss_real + D_loss_fake\n",
    "# G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(D_logit_fake, tf.ones_like(D_logit_fake)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'GEN/fully_connected/weights:0' shape=(100, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'GEN/fully_connected/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'GEN/fully_connected_1/weights:0' shape=(128, 784) dtype=float32_ref>,\n",
       " <tf.Variable 'GEN/fully_connected_1/biases:0' shape=(784,) dtype=float32_ref>,\n",
       " <tf.Variable 'DES/fully_connected/weights:0' shape=(784, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'DES/fully_connected/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'DES/fully_connected_1/weights:0' shape=(128, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'DES/fully_connected_1/biases:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'wrap/DES/fully_connected/weights:0' shape=(784, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'wrap/DES/fully_connected/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'wrap/DES/fully_connected_1/weights:0' shape=(128, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'wrap/DES/fully_connected_1/biases:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
